{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Investigación de Jax**"
      ],
      "metadata": {
        "id": "2g8BWsdabZnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo práctico.**"
      ],
      "metadata": {
        "id": "3wBKP-TXiiM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para este ejemplo usaremos el dataset de MNIST de números manuscritos, para poder ver las diferencias entre Jax, Tensorflow y PyTorch. Crearemos un modelo para predecir y sacaremos las conclusiones de como trabaja cada uno."
      ],
      "metadata": {
        "id": "O9Hj6ourl_Jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LjUlgRQuJl3",
        "outputId": "c8fd8597-b311-4a66-ebf9-870ff1357ccb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Jax"
      ],
      "metadata": {
        "id": "qRKqrkEWkXOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, random\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# Cargar MNIST desde Keras\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train = jnp.array(x_train) / 255.0\n",
        "x_train = x_train[..., None]  # (N,H,W,1)\n",
        "y_train = jnp.array(y_train)\n",
        "\n",
        "x_test = jnp.array(x_test) / 255.0\n",
        "x_test = x_test[..., None]\n",
        "y_test = jnp.array(y_test)\n",
        "\n",
        "# Función para generar mini-batches\n",
        "def get_batches(x, y, batch_size=128):\n",
        "    n = x.shape[0]\n",
        "    for i in range(0, n, batch_size):\n",
        "        yield x[i:i+batch_size], y[i:i+batch_size]\n",
        "\n",
        "# Inicializar parámetros\n",
        "def init_params(key):\n",
        "    keys = jax.random.split(key, 6)\n",
        "    return {\n",
        "        \"w1\": jax.random.normal(keys[0], (3,3,1,32)) * 0.1,\n",
        "        \"w2\": jax.random.normal(keys[1], (3,3,32,64)) * 0.1,\n",
        "        \"w3\": jax.random.normal(keys[2], (36864,128)) * 0.1,\n",
        "        \"w4\": jax.random.normal(keys[3], (128,10)) * 0.1,\n",
        "    }\n",
        "\n",
        "# Modelo convolucional\n",
        "def model(params, x):\n",
        "    x = jax.lax.conv_general_dilated(\n",
        "        x, params[\"w1\"], (1,1), \"VALID\",\n",
        "        dimension_numbers=(\"NHWC\",\"HWIO\",\"NHWC\")\n",
        "    )\n",
        "    x = jax.nn.relu(x)\n",
        "\n",
        "    x = jax.lax.conv_general_dilated(\n",
        "        x, params[\"w2\"], (1,1), \"VALID\",\n",
        "        dimension_numbers=(\"NHWC\",\"HWIO\",\"NHWC\")\n",
        "    )\n",
        "    x = jax.nn.relu(x)\n",
        "\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    x = jax.nn.relu(x @ params[\"w3\"])\n",
        "    return x @ params[\"w4\"]\n",
        "\n",
        "# Pérdida (MSE)\n",
        "def loss_fn(params, x, y):\n",
        "    logits = model(params, x)\n",
        "    labels = jax.nn.one_hot(y, 10)\n",
        "    return jnp.mean(jnp.sum((logits - labels) ** 2, axis=1))\n",
        "\n",
        "# Entrenamiento\n",
        "params = init_params(jax.random.PRNGKey(0))\n",
        "lr = 0.01\n",
        "epochs = 1\n",
        "\n",
        "@jax.jit\n",
        "def train_step(params, images, labels):\n",
        "    grads = grad(loss_fn)(params, images, labels)\n",
        "    return jax.tree_util.tree_map(lambda p, g: p - lr * g, params, grads)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{epochs} ---\")\n",
        "    for i, (images, labels) in enumerate(get_batches(x_train, y_train, batch_size=128)):\n",
        "        params = train_step(params, images, labels)\n",
        "        if i % 100 == 0: # Cada 100 batches muestra el progreso\n",
        "            current_loss = loss_fn(params, images, labels)\n",
        "            print(f\"  Batch {i+1}, Loss: {current_loss:.4f}\")\n",
        "\n",
        "# Prueba\n",
        "logits_test = model(params, x_test[:10])\n",
        "preds = jnp.argmax(logits_test, axis=1)\n",
        "print(\"\\nPredicciones:\", preds)\n",
        "print(\"Etiquetas reales:\", y_test[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DRQ98FWkYlZ",
        "outputId": "017b2671-0fae-4b13-cfda-e81104a58f96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting JAX training. The first step will involve JIT compilation, which may take some time.\n",
            "\n",
            "--- Epoch 1/1 ---\n",
            "  Batch 1, Loss: 51.3541\n",
            "  Batch 101, Loss: 0.8574\n",
            "  Batch 201, Loss: 0.7220\n",
            "  Batch 301, Loss: 0.6174\n",
            "  Batch 401, Loss: 0.5756\n",
            "\n",
            "Predicciones: [7 2 1 0 4 1 8 4 6 9]\n",
            "Etiquetas reales: [7 2 1 0 4 1 4 9 5 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensorflow"
      ],
      "metadata": {
        "id": "nIiqxVAHklfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Cargar MNIST\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "\n",
        "# Modelo CNN\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax') # 10 clases para MNIST\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "    loss='sparse_categorical_crossentropy', # Para etiquetas enteras\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Entrenamiento\n",
        "print(\"\\n--- Entrenamiento TensorFlow ---\")\n",
        "model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=1,\n",
        "    batch_size=128,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluación\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"TensorFlow - Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Predicciones de ejemplo\n",
        "logits_test = model.predict(x_test[:10])\n",
        "preds = tf.argmax(logits_test, axis=1)\n",
        "print(\"\\nPredicciones (TensorFlow):\", preds.numpy())\n",
        "print(\"Etiquetas reales (TensorFlow):\", y_test[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9flbsQvknbV",
        "outputId": "dce3875f-f65e-4c0e-d3ce-79e56d05956f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Entrenamiento TensorFlow ---\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 354ms/step - accuracy: 0.8548 - loss: 0.5382\n",
            "TensorFlow - Test Loss: 0.0774, Test Accuracy: 0.9751\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296ms/step\n",
            "\n",
            "Predicciones (TensorFlow): [7 2 1 0 4 1 4 9 5 9]\n",
            "Etiquetas reales (TensorFlow): [7 2 1 0 4 1 4 9 5 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pytorch"
      ],
      "metadata": {
        "id": "I0zYPpDFk0mN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Cargar MNIST\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# Modelo CNN para PyTorch\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(64 * 24 * 24, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu1(self.conv1(x))\n",
        "        x = self.relu2(self.conv2(x))\n",
        "        x = x.view(-1, 64 * 24 * 24)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = Net()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Entrenamiento\n",
        "print(\"\\n--- Entrenamiento PyTorch ---\")\n",
        "epochs = 1\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0: # Mostrar progreso\n",
        "            print(f'  Epoch {epoch+1}, Batch {batch_idx*len(data)}/{len(train_loader.dataset)} Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluación\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        output = model(data)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"PyTorch - Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Predicciones de ejemplo\n",
        "data, target = next(iter(test_loader))\n",
        "output = model(data[:10])\n",
        "preds = torch.argmax(output, axis=1)\n",
        "print(\"\\nPredicciones (PyTorch):\", preds.numpy())\n",
        "print(\"Etiquetas reales (PyTorch):\", target[:10].numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLMyCHVylLxI",
        "outputId": "4945ebcf-2d5a-4974-e173-f2ceb98fa51f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 22.4MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 619kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 5.61MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.78MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Entrenamiento PyTorch ---\n",
            "  Epoch 1, Batch 0/60000 Loss: 2.3002\n",
            "  Epoch 1, Batch 12800/60000 Loss: 0.1568\n",
            "  Epoch 1, Batch 25600/60000 Loss: 0.1032\n",
            "  Epoch 1, Batch 38400/60000 Loss: 0.1191\n",
            "  Epoch 1, Batch 51200/60000 Loss: 0.1207\n",
            "PyTorch - Test Accuracy: 0.9610\n",
            "\n",
            "Predicciones (PyTorch): [7 2 1 0 4 1 4 9 6 9]\n",
            "Etiquetas reales (PyTorch): [7 2 1 0 4 1 4 9 5 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tras los tres ejemplos, lo que más cabe destacar es la diferencia que hay en la sintaxis y forma de trabajar de JAX en comparación a TensorFlow y PyTorch. JAX es muy matemático y la mayoría de pasos tienes que hacerlos tú, por lo que se siente muy manual. Por otro lado, en TensorFlow y PyTorch tenemos la ventaja de que esos modelos ya están creados. Bajo mi experiencia, se ve bastante más complejo JAX al tener más libertad y tener que crear por ti mismo facilidades que TensorFlow y PyTorch ya traen por defecto.\n",
        "\n",
        "Podemos concluir y demostrar que como dijimos, JAX se centra en la programación funcional y que el usuario pueda controlar más a fondo el flujo, mientras que los otros dos priorizan la productividad del desarrollador."
      ],
      "metadata": {
        "id": "arQ8oEWakegL"
      }
    }
  ]
}